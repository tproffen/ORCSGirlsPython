{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tproffen/ORCSGirlsPython/blob/master/LLMs/NextWordPrediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From <a href=\"https://www.geeksforgeeks.org/next-word-prediction-with-deep-learning-in-nlp/\">this article</a>."
      ],
      "metadata": {
        "id": "nOFoPqsUYazn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Modules\n",
        "\n",
        "Here we simply import all the modules we need for our code. Make sure you run this cell."
      ],
      "metadata": {
        "id": "xdESYZiRkEZM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REhRO6uS2sMy"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import regex as re\n",
        "import requests"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizing the text\n",
        "\n",
        "Remember the example, we need to run words into numbers and organize the text so we have a part of the text as input and the next word as the label or correct answer. This routine does that and we will use it later to explore how the text is turned into numbers."
      ],
      "metadata": {
        "id": "9dpn1HVGkR3r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_text(text):\n",
        "\n",
        "  # Splitting the text into sentences using delimiters like '.', '?', and '!'\n",
        "  sentences = [sentence.strip() for sentence in re.split(r'(?<=[.!?])\\s+', text) if sentence.strip()]\n",
        "\n",
        "  # Tokenize the text data (turning into a number for each different word)\n",
        "  tokenizer.fit_on_texts(sentences)\n",
        "  total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "  # Create input sequences\n",
        "  input_sequences = []\n",
        "  for line in sentences:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "      n_gram_sequence = token_list[:i+1]\n",
        "      input_sequences.append(n_gram_sequence)\n",
        "\n",
        "  # Pad sequences and split into predictors and label\n",
        "  # Because of the math, all the number lists need to have the same length, so we are adding 0's to make them the same length\n",
        "  max_sequence_len = max([len(seq) for seq in input_sequences])\n",
        "  input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "  # X - contains a list of sentences which contain a list of tokens (or words)\n",
        "  # y - is the corresponding list of predicted next words\n",
        "  # This is used for training\n",
        "  X, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
        "\n",
        "  # Convert target data to one-hot encoding\n",
        "  y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n",
        "\n",
        "  return (X,y,max_sequence_len,total_words)"
      ],
      "metadata": {
        "id": "aK-XLMd9lFpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizing\n",
        "\n",
        "With our routine, we can now see how the tokenizer works."
      ],
      "metadata": {
        "id": "lVC5Ci4xmZTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()"
      ],
      "metadata": {
        "id": "X04uGE_7vlis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is our text - you can change it if you like!\n",
        "text = \"Math is super cool. So am I!\"\n",
        "\n",
        "# Let's tokenize :)\n",
        "(X,y,max_sequence_len,total_words) = tokenize_text(text)"
      ],
      "metadata": {
        "id": "h-DTozAUmk4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see what we have. Feel free to modify the code below to print different sentences or values."
      ],
      "metadata": {
        "id": "xvjAWPu0nDSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Just printing X and y\n",
        "print (X, y)"
      ],
      "metadata": {
        "id": "FM609NRHnLvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hmmm, not so use ful. Which word is which?\n",
        "for i in range(1,len(tokenizer.index_word)+1):\n",
        "  print (i,tokenizer.index_word[i])"
      ],
      "metadata": {
        "id": "SLfqivqknWhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop over the sequences and print them with tokens and words\n",
        "for s in range(len(X)):\n",
        "  out = ''\n",
        "  for word in X[s]:\n",
        "      if word > 0:\n",
        "        out+=tokenizer.index_word[word]+' '\n",
        "  print(f\"{out} -- \\033[31m {tokenizer.index_word[y[s].argmax()]}\\033[30m\")"
      ],
      "metadata": {
        "id": "OCt-0UMNoKpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Space for your exploration code .."
      ],
      "metadata": {
        "id": "Zt5Di2rioL9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training on the pizza text\n",
        "\n",
        "Next we read and tokenize the <a href=\"https://raw.githubusercontent.com/tproffen/ORCSGirlsPython/refs/heads/master/LLMs/pizza.txt\">pizza input text</a> and train the LLM. <b>Note this will take some time.</b>"
      ],
      "metadata": {
        "id": "ynjIs47Kn50y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()"
      ],
      "metadata": {
        "id": "O4nHtNWsxurD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/tproffen/ORCSGirlsPython/refs/heads/master/LLMs/pizza.txt\"\n",
        "response = requests.get(url)\n",
        "text = response.text\n",
        "\n",
        "(X,y,max_sequence_len,total_words) = tokenize_text(text)"
      ],
      "metadata": {
        "id": "_seM9KRQrqtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 10, input_length=max_sequence_len-1))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "-2O9gDY227_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.fit(X, y, epochs=50, verbose=1)"
      ],
      "metadata": {
        "id": "Bd30WNiZ29V5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using the trained model\n",
        "\n",
        "Now we can use the model and predict the next words based on the pizza text we useed to train :)"
      ],
      "metadata": {
        "id": "CjSDivExsZz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate next word predictions - feel free to change these\n",
        "seed_text = \"The best pizza is \"\n",
        "next_words = 20\n",
        "\n",
        "for _ in range(next_words):\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\tpredicted_probs = model.predict(token_list)\n",
        "\tpredicted_word = tokenizer.index_word[np.argmax(predicted_probs)]\n",
        "\tseed_text += \" \" + predicted_word\n",
        "\n",
        "print(\"Next predicted words:\", seed_text)"
      ],
      "metadata": {
        "id": "NFxXKAKn2-yk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ptzQHtL-UVIE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}